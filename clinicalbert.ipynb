{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这里使用了筛选出来的data\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# 设置标签列名（根据你实际的标签列设置）\n",
    "label_cols = [\n",
    "    \"Enlarged Cardiomediastinum\", \"Cardiomegaly\", \"Lung Opacity\",\n",
    "    \"Lung Lesion\", \"Edema\", \"Consolidation\", \"Pneumonia\",\n",
    "    \"Atelectasis\", \"Pneumothorax\", \"Pleural Effusion\",\n",
    "    \"Pleural Other\", \"Fracture\", \"Support Devices\"\n",
    "]\n",
    "\n",
    "# ⬇️ 主清洗函数：填补 text 为 'NA'，labels 的 NaN 为 0，并合并为 list\n",
    "def clean_dataframe(df):\n",
    "    # 填补空文本为 'NA'（字符串）\n",
    "    df[\"text\"] = df[\"text\"].fillna(\"NA\")\n",
    "\n",
    "    # 填补标签空值为 0，确保类型为 int\n",
    "    df[label_cols] = df[label_cols].fillna(0).astype(float)\n",
    "\n",
    "    # 合并标签为 list[int] 放入 'labels' 列中\n",
    "    df[\"labels\"] = df[label_cols].values.tolist()\n",
    "\n",
    "    return df\n",
    "\n",
    "# ⬇️ 载入并处理三个数据集\n",
    "def load_and_clean_all(train_path, dev_path, test_path):\n",
    "    df_train = clean_dataframe(pd.read_csv(train_path))\n",
    "    df_dev   = clean_dataframe(pd.read_csv(dev_path))\n",
    "    df_test  = clean_dataframe(pd.read_csv(test_path))\n",
    "\n",
    "    # 转换为 HuggingFace Datasets\n",
    "    ds_train = Dataset.from_pandas(df_train)\n",
    "    ds_dev   = Dataset.from_pandas(df_dev)\n",
    "    ds_test  = Dataset.from_pandas(df_test)\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": ds_train,\n",
    "        \"validation\": ds_dev,\n",
    "        \"test\": ds_test\n",
    "    })\n",
    "\n",
    "# ⬇️ 替换为你自己的路径\n",
    "train_file = \"/content/text_train_data.csv\"\n",
    "dev_file   = \"/content/text_val_data.csv\"\n",
    "test_file  = \"/content/text_test_data.csv\"\n",
    "\n",
    "# 执行清洗和数据集构建\n",
    "raw_datasets = load_and_clean_all(train_file, dev_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "num_labels = len(label_cols)  # 多标签分类的标签数量\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"  # ✅ 使用 BCEWithLogitsLoss\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256  # 可按实际需求调整\n",
    "    )\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def multi_label_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "\n",
    "    # ✅ 概率输出（多标签 sigmoid）\n",
    "    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "    y_true = labels\n",
    "    y_pred = (probs > 0.5).astype(int)\n",
    "\n",
    "    # ✅ 基本指标\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    micro_f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # ✅ AUC（注意要用概率）\n",
    "    try:\n",
    "        macro_auc = roc_auc_score(y_true, probs, average=\"macro\")\n",
    "        micro_auc = roc_auc_score(y_true, probs, average=\"micro\")\n",
    "    except ValueError:\n",
    "        macro_auc = -1\n",
    "        micro_auc = -1\n",
    "\n",
    "    # ✅ 每个标签单独的 AUC\n",
    "    num_classes = y_true.shape[1]\n",
    "    per_class_auc = []\n",
    "    for i in range(num_classes):\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true[:, i], probs[:, i])\n",
    "        except ValueError:\n",
    "            auc = -1\n",
    "        per_class_auc.append(auc)\n",
    "\n",
    "    # ✅ 构造返回字典\n",
    "    result = {\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_auc\": macro_auc,\n",
    "        \"micro_auc\": micro_auc,\n",
    "    }\n",
    "\n",
    "    # ✅ 加入每类 AUC（默认命名 auc_class_0, auc_class_1,...）\n",
    "    for i, auc in enumerate(per_class_auc):\n",
    "        result[f\"auc_class_{i}\"] = auc\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3,  # 可按需调整\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=multi_label_metrics\n",
    ")\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# ✅ 模型路径（你训练好的模型）\n",
    "model_path = \"./checkpoints/checkpoint-13683\"  # 替换成你的路径\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# ✅ 标签名列表（你要替换成自己的标签顺序）\n",
    "label_cols = [\n",
    "    \"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"Effusion\",\n",
    "    \"Emphysema\", \"Fibrosis\", \"Hernia\", \"Infiltration\", \"Mass\",\n",
    "    \"Nodule\", \"Pleural Thickening\", \"Pneumonia\", \"Pneumothorax\"\n",
    "]\n",
    "\n",
    "# ✅ 输入报告\n",
    "text = \"the left lower lobe. No pleural effusion or pneumothorax. Mild atelectasis noted.\"\n",
    "\n",
    "# ✅ 编码 + 推理\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.sigmoid(outputs.logits).squeeze().cpu().numpy()  # 多标签用 sigmoid\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "\n",
    "# ✅ 转换成字典格式\n",
    "processed_report = {label: int(pred) for label, pred in zip(label_cols, preds)}\n",
    "\n",
    "# ✅ 打印结果\n",
    "from pprint import pprint\n",
    "pprint(processed_report)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
