{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9TCwhBz4LXB",
        "outputId": "ae86d44b-d903-459e-faf3-b30c5cc66efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Device name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SJVIPl4p4fPt"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lBmk5jh34KPn"
      },
      "outputs": [],
      "source": [
        "# Report labels for the CheXpert dataset\n",
        "report_label = [\n",
        "    'Atelectasis',\n",
        "    'Cardiomegaly',\n",
        "    'Consolidation',\n",
        "    'Edema',\n",
        "    'Enlarged Cardiomediastinum',\n",
        "    'Fracture',\n",
        "    'Lung Lesion',\n",
        "    'Lung Opacity',\n",
        "    'Pleural Effusion',\n",
        "    'Pneumonia',\n",
        "    'Pneumothorax',\n",
        "    'Pleural Other',\n",
        "    'Support Devices',\n",
        "    'No Finding'\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUAQcLv84KPn"
      },
      "source": [
        "# use processed/clean text training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re-b2wEM4KPo"
      },
      "source": [
        "# model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAcSCc3c4KPo"
      },
      "source": [
        "Task 2: convert free-text radiology reports into 14 structured binary labels (presence/absence of specific conditions)<br>\n",
        "\n",
        "Workflow:<br>\n",
        "\n",
        "1. train a text-based classifier on text_train_data.h5<br>\n",
        "2. validate its performance on text_val_data.h5<br>\n",
        "3. run inference on text_test_data.h5 to produce label predictions<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### h5py datasets"
      ],
      "metadata": {
        "id": "6UfzarU-HN1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### use .csv datasets"
      ],
      "metadata": {
        "id": "i0ZEDccdE3ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load .csv dataset\n",
        "train_df = pd.read_csv(\"text_train_data.csv\")\n",
        "val_df = pd.read_csv(\"text_val_data.csv\")\n",
        "test_df = pd.read_csv(\"text_test_data.csv\")"
      ],
      "metadata": {
        "id": "gyp4sL09E1Vs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O7aFgc-4KPo"
      },
      "source": [
        "## processing\n",
        "\n",
        "Cased model: BioBERT learns upper and lower cases differently --> keep original casing<br>\n",
        "To avoid truncation and potential loss of meaning in BERT-based models, apply sentence splitting<br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define preprocessing functions\n",
        "def clean_report(report):\n",
        "    report = re.sub(r'_{2,}', '<UNK>', report)\n",
        "    report = re.sub(r'\\s+', ' ', report).strip()\n",
        "    return report\n",
        "\n",
        "def simple_sentence_split(text):\n",
        "    return [s.strip() for s in re.split(r'(?<=[.?!])\\s+(?=[A-Z0-9])', text) if s.strip()]\n",
        "\n",
        "def join_sentences(sentences, max_sentences=6):\n",
        "    return ' '.join(sentences[:max_sentences])\n",
        "\n",
        "def preprocess(df):\n",
        "    cleaned = df[\"text\"].apply(clean_report)\n",
        "    split = cleaned.apply(simple_sentence_split)\n",
        "    joined = split.apply(join_sentences)\n",
        "    return joined.tolist(), df[\"file_name\"].tolist(), df.iloc[:, 2:].values.astype('float32')  # labels are all columns from 3rd onward\n",
        "\n",
        "# Assign to original variable names\n",
        "texts, patient_ids, labels = preprocess(train_df)\n",
        "texts_val, patient_ids_val, labels_val = preprocess(val_df)\n",
        "texts_test, patient_ids_test, labels_test = preprocess(test_df)"
      ],
      "metadata": {
        "id": "bcn2FVdTwabU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY8gtbQf4KPo"
      },
      "source": [
        "## sentence splitting\n",
        "From what I understand, tokenization has size limit<br>\n",
        "To avoid truncation and potential loss of meaning in BERT-based models, I want to try sentence splitting<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ITaj8Rh4KPp"
      },
      "source": [
        "## define & train models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define tokeniser"
      ],
      "metadata": {
        "id": "DCxoXNoky_CQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### updated for .csv datasets"
      ],
      "metadata": {
        "id": "68VObxr7IL69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "define tokenizer for t"
      ],
      "metadata": {
        "id": "6JudY4qrzm1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "\n",
        "tokenized_train = tokenizer(\n",
        "    texts,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=256,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "tokenized_val = tokenizer(\n",
        "    texts_val,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=256,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "tokenized_test = tokenizer(\n",
        "    texts_test,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=256,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "# Extract tensors\n",
        "input_ids = tokenized_train['input_ids']\n",
        "attention_mask = tokenized_train['attention_mask']"
      ],
      "metadata": {
        "id": "P_qOjqKqIQN8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "d1e2920299c14d5bb9529b3eae2a1647",
            "446cb952aeaf4679b263d0111c18fd3c",
            "a05a690b4f174832bd2d2939252f164b",
            "2fd107ab18704f50a61d3b641e2a8390",
            "8def1bc2197e4ef99215aa2e56cab9c7",
            "c858dcfd1c6b488faeab27cbf1b831d0",
            "4a4393307588490ab828723c786b1d07",
            "0fdd8d69d8a8496c9af10fb050135e37",
            "d61ac2bbffaf4eacbc8afc97ffe6d65f",
            "5599e35ce08a461fa22c3667d4c52066",
            "31548515c7224f0e83d207ec4b3fdca2"
          ]
        },
        "id": "eBEKQ-f84KPp",
        "outputId": "6d0d73ec-4188-4285-db1f-8078d4542e3a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1e2920299c14d5bb9529b3eae2a1647"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# use pre-trained BioBERT model\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    num_labels=13,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "define Dataset functions"
      ],
      "metadata": {
        "id": "9dc7pNWr2u9Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rXOwwA1l4KPp"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class RadiologyReportDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(label, dtype=torch.float32)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FXf_JnL64KPp"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# set batch size\n",
        "batch_size = 8\n",
        "\n",
        "# create training dataset\n",
        "# (the same as the original dataset, as test/train was already split)\n",
        "train_dataset = RadiologyReportDataset(texts, labels, tokenizer)\n",
        "\n",
        "# create data loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FELc5e44KPq",
        "outputId": "a2d8258c-ca30-43a5-ab5e-3e61f7a3431f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 512])\n",
            "torch.Size([8, 13])\n"
          ]
        }
      ],
      "source": [
        "# sanity test\n",
        "\n",
        "for batch in train_loader:\n",
        "    print(batch['input_ids'].shape)   # should be [batch_size, 512]\n",
        "    print(batch['labels'].shape)      # should be [batch_size, 13]\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc6IfeEG4KPq",
        "outputId": "48aade46-95c5-40b3-ff3a-a1192a53655c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Using GPU: Tesla T4\n",
            "GPU memory usage:\n",
            "Allocated: 0.0 MB\n",
            "Cached:    0.0 MB\n"
          ]
        }
      ],
      "source": [
        "# GPU sanity check\n",
        "import torch\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "    print(\"GPU memory usage:\")\n",
        "    print(\"Allocated:\", round(torch.cuda.memory_allocated(0)/1024**2, 1), \"MB\")\n",
        "    print(\"Cached:   \", round(torch.cuda.memory_reserved(0)/1024**2, 1), \"MB\")\n",
        "else:\n",
        "    print(\"Running on CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### establish baseline\n",
        "batch_size = 8 <br>\n",
        "epoch = 1 <br>\n",
        "max_length = 300 <br>\n",
        "mask NaN --> label -1"
      ],
      "metadata": {
        "id": "a0Lo9iK8V9n9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkqKPjeG4KPq"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# mask NaN labels (the missing values in medical reports may contain uncertainty)\n",
        "labels = np.nan_to_num(labels, nan=-1.0)\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "total_loss = 0\n",
        "\n",
        "learning_rate = 2e-4\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "for batch in train_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Masked BCEWithLogitsLoss\n",
        "    loss = loss_fn(logits, labels)\n",
        "    mask = (labels != -1).float()\n",
        "    masked_loss = (loss * mask).sum() / mask.sum()\n",
        "\n",
        "    masked_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += masked_loss.item()\n",
        "\n",
        "avg_train_loss = total_loss / len(train_loader)\n",
        "print(f\"epoch 1 complete — average train loss: {avg_train_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if max_length can be set to 300 without information loss\n",
        "# token lengths for each sample\n",
        "token_lengths = [len(tokenizer.encode(text, truncation=False)) for text in joined_texts]\n",
        "\n",
        "# summary stats\n",
        "print(\"Max token length:\", max(token_lengths))\n",
        "print(\"95th percentile:\", np.percentile(token_lengths, 95))\n",
        "print(\"Average token length:\", np.mean(token_lengths))"
      ],
      "metadata": {
        "id": "UQ_kreQ5FPAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model result\n",
        "torch.save(model.state_dict(), \"biobert_epoch1.pt\")"
      ],
      "metadata": {
        "id": "CEAzHwa-sAEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"biobert_epoch1.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nEBfMjhlWcYB",
        "outputId": "cda46453-e766-4952-fdfd-5ad46b13e53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0e71065e-dd13-4d6a-a81c-efda8df08793\", \"biobert_epoch1.pt\", 433364091)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model performance using val_data (after 1 epoch)\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "import torch\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_targets = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs > 0.5).float()\n",
        "\n",
        "        mask = (labels != -1)\n",
        "        masked_preds = preds[mask]\n",
        "        masked_labels = labels[mask]\n",
        "\n",
        "        # collect batch predictions for NaN but move them to CPU\n",
        "        all_preds.append(masked_preds.cpu())\n",
        "        all_targets.append(masked_labels.cpu())\n",
        "\n",
        "# combine predictions and labels\n",
        "y_pred = torch.cat(all_preds).numpy()\n",
        "y_true = torch.cat(all_targets).numpy()\n",
        "\n",
        "# compute macro F1 score, accuracy\n",
        "val_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "val_acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(f\"validation macro F1 Score (after 1 epoch): {val_f1:.4f}\")\n",
        "print(f\"validation accuracy (after 1 epoch): {val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2GEBlQ-HADp",
        "outputId": "4ccbb7c3-962f-4591-bdf9-a728df5f1324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation macro F1 Score (after 1 epoch): 0.4846\n",
            "validation accuracy (after 1 epoch): 0.9403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BioBERT (unmasked)"
      ],
      "metadata": {
        "id": "FO2ZT-zDYRBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check again\n",
        "print(f\"texts: {len(texts)}, labels: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3DEm7qdsyqm",
        "outputId": "6fa3187b-89b1-4c1d-804b-879a664986dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texts: 36484, labels: (36484, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# 1: convert NaNs to soft targets (0.5)\n",
        "labels = np.nan_to_num(labels, nan=0.5)\n",
        "\n",
        "# 2: define dataset and dataloader\n",
        "train_dataset = RadiologyReportDataset(texts, labels, tokenizer, max_length=256)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# 3: define model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    num_labels=13,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 4: training parameter setting\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# 5: train for multiple epochs\n",
        "num_epochs = 3  # fine-tune\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        label_batch = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = loss_fn(logits, label_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} complete — avg Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # save checkpoint\n",
        "    checkpoint_path = f\"biobert_soft_epoch{epoch+1}.pt\"\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "    # download last checkpoint (optional in Colab)\n",
        "    if epoch + 1 == num_epochs:\n",
        "        from google.colab import files\n",
        "        files.download(checkpoint_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "hHfrND0-K6gD",
        "outputId": "4e8aa5bf-dd68-440b-a7bc-7291f16e0caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n",
            "Epoch 1 complete — avg Loss: 0.1207\n",
            "\n",
            "Epoch 2/3\n",
            "Epoch 2 complete — avg Loss: 0.0945\n",
            "\n",
            "Epoch 3/3\n",
            "Epoch 3 complete — avg Loss: 0.0858\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_eb288811-33ff-4e45-8758-ecc2c5a6b998\", \"biobert_soft_epoch3.pt\", 433365180)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### validation & test on BioBERT (unmasked)"
      ],
      "metadata": {
        "id": "7WNYF4tHHFnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define validate function\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, classification_report\n",
        "\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_probs, all_targets = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > 0.5).float()\n",
        "\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_probs.append(probs.cpu())\n",
        "            all_targets.append(labels.cpu())\n",
        "\n",
        "    y_pred = torch.cat(all_preds).cpu().numpy().astype(int)\n",
        "    y_prob = torch.cat(all_probs).cpu().numpy()\n",
        "    y_true = torch.cat(all_targets).cpu().numpy().astype(int)\n",
        "\n",
        "    # classification Report\n",
        "    print(\"Classification Report: Validation\")\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "    # F1\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
        "\n",
        "    # AUROC\n",
        "    roc_macro = roc_auc_score(y_true, y_prob, average='macro')\n",
        "    roc_micro = roc_auc_score(y_true, y_prob, average='micro')\n",
        "\n",
        "    print(f\"F1 Macro: {f1_macro:.4f}\")\n",
        "    print(f\"F1 Micro: {f1_micro:.4f}\")\n",
        "    print(f\"AUROC Macro: {roc_macro:.4f}\")\n",
        "    print(f\"AUROC Micro: {roc_micro:.4f}\")"
      ],
      "metadata": {
        "id": "eNaJ8BJVHFTS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save upload of saved weight\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "x15WIysFBqkV",
        "outputId": "bc982bcd-076a-47af-d35f-6003287e08f8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a6c8b1cb-e165-4a4c-8dea-b0ced5d92bde\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a6c8b1cb-e165-4a4c-8dea-b0ced5d92bde\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving biobert_soft_epoch3.pt to biobert_soft_epoch3.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved models into model for evaluation\n",
        "model.load_state_dict(torch.load(\"biobert_soft_epoch3.pt\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v0nuPJiMhSS",
        "outputId": "f802484f-db3d-4706-bb5f-fbeb481dff68"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=13, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create validation & test loaders"
      ],
      "metadata": {
        "id": "KtNPGiP-5Jgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create validation and test datasets directly from raw texts and labels\n",
        "val_dataset = RadiologyReportDataset(texts_val, labels_val, tokenizer, max_length=256)\n",
        "test_dataset = RadiologyReportDataset(texts_test, labels_test, tokenizer, max_length=256)\n",
        "\n",
        "# Wrap in DataLoaders\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "0hZCQHYq5JB7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, val_loader, device)\n",
        "evaluate_model(model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeJPSXx8N1Hg",
        "outputId": "e4550484-40df-4113-99b4-b38b4537dccb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report: Validation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6588    0.4108    0.5060       409\n",
            "           1     0.7835    0.4222    0.5487       360\n",
            "           2     0.7174    0.3626    0.4818        91\n",
            "           3     0.7286    0.4811    0.5795       106\n",
            "           4     0.5000    0.0145    0.0282        69\n",
            "           5     0.7143    0.2941    0.4167       102\n",
            "           6     0.6437    0.3709    0.4706       151\n",
            "           7     0.7064    0.5441    0.6147       544\n",
            "           8     0.7275    0.7880    0.7565       349\n",
            "           9     0.6364    0.1892    0.2917        37\n",
            "          10     0.7129    0.3512    0.4706       205\n",
            "          11     0.4828    0.5714    0.5234        49\n",
            "          12     0.6333    0.3858    0.4795       197\n",
            "\n",
            "   micro avg     0.6983    0.4665    0.5593      2669\n",
            "   macro avg     0.6650    0.3989    0.4745      2669\n",
            "weighted avg     0.6950    0.4665    0.5437      2669\n",
            " samples avg     0.1799    0.1371    0.1484      2669\n",
            "\n",
            "F1 Macro: 0.4745\n",
            "F1 Micro: 0.5593\n",
            "AUROC Macro: 0.9332\n",
            "AUROC Micro: 0.9511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report: Validation\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7034    0.4148    0.5219       446\n",
            "           1     0.7080    0.4000    0.5112       400\n",
            "           2     0.7297    0.3600    0.4821        75\n",
            "           3     0.7527    0.5691    0.6481       123\n",
            "           4     0.3333    0.0143    0.0274        70\n",
            "           5     0.7021    0.4024    0.5116        82\n",
            "           6     0.5769    0.3750    0.4545       120\n",
            "           7     0.7040    0.5243    0.6010       576\n",
            "           8     0.6886    0.7710    0.7275       393\n",
            "           9     0.7000    0.1591    0.2593        44\n",
            "          10     0.7016    0.3850    0.4971       226\n",
            "          11     0.5397    0.6182    0.5763        55\n",
            "          12     0.5000    0.3134    0.3853       201\n",
            "\n",
            "   micro avg     0.6792    0.4685    0.5545      2811\n",
            "   macro avg     0.6415    0.4082    0.4772      2811\n",
            "weighted avg     0.6724    0.4685    0.5395      2811\n",
            " samples avg     0.1899    0.1488    0.1591      2811\n",
            "\n",
            "F1 Macro: 0.4772\n",
            "F1 Micro: 0.5545\n",
            "AUROC Macro: 0.9345\n",
            "AUROC Micro: 0.9505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "akQOs5R3QD1P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d1e2920299c14d5bb9529b3eae2a1647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_446cb952aeaf4679b263d0111c18fd3c",
              "IPY_MODEL_a05a690b4f174832bd2d2939252f164b",
              "IPY_MODEL_2fd107ab18704f50a61d3b641e2a8390"
            ],
            "layout": "IPY_MODEL_8def1bc2197e4ef99215aa2e56cab9c7"
          }
        },
        "446cb952aeaf4679b263d0111c18fd3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c858dcfd1c6b488faeab27cbf1b831d0",
            "placeholder": "​",
            "style": "IPY_MODEL_4a4393307588490ab828723c786b1d07",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "a05a690b4f174832bd2d2939252f164b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fdd8d69d8a8496c9af10fb050135e37",
            "max": 435780550,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d61ac2bbffaf4eacbc8afc97ffe6d65f",
            "value": 435780550
          }
        },
        "2fd107ab18704f50a61d3b641e2a8390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5599e35ce08a461fa22c3667d4c52066",
            "placeholder": "​",
            "style": "IPY_MODEL_31548515c7224f0e83d207ec4b3fdca2",
            "value": " 436M/436M [00:01&lt;00:00, 448MB/s]"
          }
        },
        "8def1bc2197e4ef99215aa2e56cab9c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c858dcfd1c6b488faeab27cbf1b831d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a4393307588490ab828723c786b1d07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fdd8d69d8a8496c9af10fb050135e37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d61ac2bbffaf4eacbc8afc97ffe6d65f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5599e35ce08a461fa22c3667d4c52066": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31548515c7224f0e83d207ec4b3fdca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}